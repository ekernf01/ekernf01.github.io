---
layout: post
title: Buying a computer for single-cell genomics
math: true
permalink: computer_for_scrna
---


Are you looking to get into single-cell genomics and wondering what kind of hardware to buy? I am also facing a hardware transition as I plan for grad school, and I wanted to share some experience from my previous job. For the past ~four years, I analyzed single-cell RNA-seq data for the [Maehr Lab at UMass Medical](https://ekernf01.github.io/about_maehrlab/), with several different datasets each containing tens of thousands of cells. I used a beefy 2016 MacBook Pro: 16GB RAM and a 512GB SSD. Though you can buy machines with 64GB of RAM as of August 2020, the less expensive ones still have 8GB or 16GB, and it's a useful point of reference to describe which common analysis tasks are possible with those specifications and which ones are not. Here's a description of when and how I started to hit the limits of my hardware.


- Storage: 
  - For one big project, it's easy to save 20GB of data files. One copy of the processed data can easily reach 3GB because it will usually contain both raw and normalized or logged counts. Realistically, there will be redundant storage: you can't pipe everything straight through from the raw count matrices, because you need to interact with the data in order to determine what type of analysis to run. I typically saved R objects or `loom` files after multiple key analysis stages. 
  - Single-cell chromatin assays and data integration tasks have escalated the problem.  The group coverage tracks created by [ArchR](https://www.archrproject.com/) can easily reach 10Gb for a modest dataset produced using commercially available kits. Data integration may involve >100k cells, so it's best to avoid storing multiple copies (outside of your usual backups, of course!).
  - For us, there were also a lot of projects: for everything we published, we had 3 or 4 similar projects centered around a single-cell dataset. Even if they are not all as big as the datasets we actively worked on, the total storage really piled up. If you feel this creeping up and you want to explore your own disk usage, definitely **try [ncdu](https://dev.yorhel.nl/ncdu)**. 
- Memory:
  - This was my number one bottleneck. If I had 32GB or 64GB of memory, I would have used it. I often have multiple R sessions or Jupyter notebooks running at once -- for instance, I might have one interactive session, one sitting idle from earlier, and one heavy task running. Usually it was fine if I took care to clean up after myself (release memory) in the interactive sessions, but whenever I had more than usual going on, I hit the limit, and that was sad.
  - For an individual session, you can fill 16GB of memory easily when **merging two large datasets** or during any operation that **converts a sparse matrix into a dense one**. I usually had matrices sitting around of dimension 30k (genes) by 10-60k (cells). Working with Seurat, my objects were typically 750MB to 4GB. The bigger matrices were $3e4\times 6e4 =1.8e9$, so two gigadoubles when stored densely. A "double" (double precision floating-point number) is 8 bytes, so that's $8 \times 1.8e9 = 14.4$ GB. Operations that yield dense matrices include the [MNN batch correction of Haghverdi et al](https://pubmed.ncbi.nlm.nih.gov/29608177/) as well as centering and scaling. There is [a way to avoid this](https://github.com/ekernf01/MatrixLazyEval), but it's not widely used, so be careful with these dense matrix operations.
- Upstream processing:
  - We largely used [cellranger](https://support.10xgenomics.com/single-cell-gene-expression/software/overview/welcome) and other similar pipelines for alignment and quantification, but we ran these on [our cluster](https://www.mghpcc.org/), not our laptops. These often use the [STAR aligner](https://pubmed.ncbi.nlm.nih.gov/23104886/), which loads a genome index requiring dozens of GB of memory (e.g. 28GB for mouse genome build `mm10`). Whether or not your pipeline involves STAR, a typical sequencing run would produce 400 million short reads, which . If you have no choice but to run this on your laptop -- I never had to -- consider using cloud computing (e.g. from [Amazon](https://aws.amazon.com/health/genomics/)) or try a faster alternative like [Kallisto](https://pachterlab.github.io/kallisto/singlecell.html).

Your choice about hardware will depend on what type of work you are planning to do. I am planning to transition towards less exploratory data analysis and more methods development. If I get to chase [the projects I am interested in](), I can afford to interact with data less intensely, and the heavy lifting can be done on AWS or computing clusters. For now, I have decided to []upgrade my personal laptop] and see how long I make it. 