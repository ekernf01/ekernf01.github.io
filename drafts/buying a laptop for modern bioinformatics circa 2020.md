Why did I have to [push my last work laptop so hard](???)? Will I have to push the next one just as hard? At [my previous job](maehr lab about post), we did almost exclusively single-cell RNA-seq analysis, with several different datasets each containing tens of thousands of cells. I used a beefy 2016 Macook Pro: 16GB RAM and a 512GB SSD. I got a good intuition for what you can and cannot do.

- Storage: 
  - For one big project, it's easy to hit 20GB. One copy of the processed data can easily reach 3GB because it will usually contain both raw and normalized or logged counts. Realistically, there will be redundant storage: you can't pipe everything straight through from the raw count matrices, because you need to interact with the data in order to determine what code to run. I typically saved R objects or `loom` files after multiple key analysis stages. 
  - Data integration efforts and single-cell chromatin assays have escalated the problem. Data integration may involve >100k cells, so it's best to avoid copying. The group coverage tracks created by [ArchR](???) can easily reach 10Gb for a modest dataset produced using commercially available kits.
  - There are also a lot of projects: for everything we actually publish, there's 3 or 4 similar datasets sitting in a drawer. Even if they are not all as big as the datasets we're actively working on, there's a long tail of small files that really builds up. If you feel this creeping up and you want to explore your own disk usage, definitely **try [ncdu](https://dev.yorhel.nl/ncdu)**. 
- Memory:
  - This was my number one bottleneck. If I could cheaply have 32GB or 64GB of memory, I would have used it. I often have multiple R sessions or Jupyter notebooks running at once -- for instance, I might have one interactive session, one sitting idle from earlier, and one heavy task running. Usually it was fine if I took care to clean up after myself (release memory) in the interactive sessions, but whenever I had more than usual going on, I hit the limit, and that was sad.
  - For an individual session, you can max out your 16GB easily when **merging two large datasets** or during any operation that **converts a sparse matrix into a dense one**. I usually had matrices sitting around of dimension 30k (genes) by 10-60k (cells). The bigger ones were $3e4\times 6e4 =1.8e9$, so two gigadoubles when stored densely. A "double" (double precision floating-point number) is 8 bytes, so that's $8 \times 1.8e9 = 14.4$GB.


I may not need to work like this in the future -- it will depend on the culture in my next lab. I heard from a postdoc in a prominent European group that it's rare to work on fewer than 3 projects on any given day, so that sounds similar to what I have been doing (harder, actually). But if I transition towards different data types or towards methods development, I may have fundamentally different needs. If I get to chase [the projects I am interested in](), I can afford to interact with data less intensely, and the heavy lifting can be done on AWS or computing clusters. That's why I decided to just upgrade my personal laptop from 2016 and see how long I make it.