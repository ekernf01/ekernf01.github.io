I"∆<p>Neural networks are exploding through AI at the same pace as single-cell technology in genomics and GWAS consortia in genetics ‚Äì that is to say, very quickly. For one example, ImageNet error rates decreased tenfold from 2010 to 2017 (<a href="http://image-net.org/challenges/talks_2017/imagenet_ilsvrc2017_v1.0.pdf">source</a>), but people are also now using neural networks to <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">play StarCraft at a superhuman level</a>, <a href="https://www.youtube.com/watch?v=kSLJriaOumA">synthesize and manipulate photorealistic images of faces</a>, and yes, also to <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5737331/">analyze single-cell genomics data</a>. This seems like a class of models I should consider learning to work with.</p>

<p>Unfortunately, I tried to train a neural net on an image labeling task back in grad school and failed miserably. This left a bad taste in my mouth, and I‚Äôd like to start fresh.</p>

<h4 id="getting-started---theory">Getting started - theory</h4>

<p>You don‚Äôt need to know any theory at all to start fitting neural networks ‚Äì see next section. But on the surface of it, neural networks are not complicated compared to a lot of statistical &amp; machine learning methods. A neural net is a just a flexible, tunable function with multiple inputs and (potentially) multiple outputs. It is composed of many sequential transformations:</p>

<p>\(NN(x) = f_n(g_n(f_{n-1}(g_{n-1}( ... f_1(g_1(x)) ... ))))\).</p>

<p>The $g$‚Äôs are linear: for example, $g(x_1, x_2)$ might be $5x_1 + 2x_2 + 4$. The $f$‚Äôs are nonlinear: for example, $f(x)$ might be a sigmoid $\frac{1}{1+\exp -x}$. Training a neural network just means choosing different values of the parameters: $5x_1 + 2x_2 + 4$ versus $-x_1 + 2x_2 + 1$, for example.</p>

<p><img src="/images/siegfried.png" alt="The sigmoid, a simple nonlinear function" /></p>

<p>To do the training strategically rather than haphazardly, it helps to take the derivative of the neural net function. This is done using a thing called ‚Äúreverse mode automatic differentiation‚Äù (or autodiff), but autodiff is actually also simple. It‚Äôs just a loop that repeatedly applies the chain rule from high school calculus: $\frac{dx}{dy} \frac{dy}{dz} = \frac{dx}{dz}$. The clever part is that it retains certain results as it goes, reusing them instead of starting from scratch at each layer; this speeds things up and saves electricity.</p>

<p>You can fit a neural network without knowing any of this, though: read on.</p>

<h4 id="getting-started---software">Getting started - software</h4>

<p>I decided to get started using the <a href="https://www.tensorflow.org/tutorials">tensorflow MNIST tutorial</a> for handwritten digit classification. The biggest hurdle was not running the model itself, which was actually a cinch; the problem was the setup. Tensorflow is a very fancy and powerful piece of software: autodiff is easier said than done, and tensorflow implements many additional features such as optimization algorithms and support for different types of hardware. So, it has certain <a href="https://www.tensorflow.org/install">installation requirements</a>. I had to update my operating system twice <code class="language-plaintext highlighter-rouge">&gt;Ô∏π&lt;</code>. After that, I had a bad time coaxing my notebook to <a href="https://anbasile.github.io/programming/2017/06/25/jupyter-venv/">find the installation</a>. (Technical aside: if you‚Äôre sloppily following the linked instructions like I did, the trick is that it‚Äôs not enough to have the <code class="language-plaintext highlighter-rouge">ipykernel</code> package installed globally. It has to be installed in the virtual environment that you are using. Virtual enviromnents contain isolated sets of packages for the Python programming language; here‚Äôs <a href="https://docs.python.org/3/library/venv.html">what</a> they are and <a href="https://medium.com/knerd/best-practices-for-python-dependency-management-cc8d1913db82">why</a> I use them.)</p>

<p>Once I got that done, the Tensorflow example code ran like a charm.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow as tf 
mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape = (28, 28)),
    tf.keras.layers.Dense(512, activation = tf.nn.relu), 
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation = tf.nn.softmax)
])
model.compile(optimizer = 'adam', 
              loss = 'sparse_categorical_crossentropy', 
              metrics = ['accuracy'])
model.fit(x_train, y_train, epochs = 1)
model.evaluate(x_test, y_test)
</code></pre></div></div>

<p>This code reports accuracy in the high nineties after training for a matter of seconds. Tensorflow is very impressive, and I might have had an easier time back in grad school if I had chosen to use it. (At the time, I was using <a href="http://deeplearning.net/software/theano/">Theano</a>, a similar tool.)</p>

<p>I did a little follow-up to explore the results. This code looks for the most confusing test example ‚Äì the one where the second best prediction is just about as good as the top prediction.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import seaborn as sns
import pandas as pd
import numpy as np


yhat = model.predict(x_test)
X = pd.DataFrame(yhat)
X['label'] = y_test
X['index'] = X.index

def get_confusion(x):
    y = x.copy()
    y.sort()
    return y[-2]
   
X['confusion'] = np.apply_along_axis(get_confusion, 1, yhat.copy())

sns.scatterplot(data = X, x = 'index', y = 'confusion', hue = 'label')
</code></pre></div></div>

<p>I added code to print the top five confusing digits so that I could pick my favorite.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X = X.sort_values('confusion')
for image in X.tail().index:
    confusing_digit = sns.heatmap( x_test[image] )
    confusing_digit.get_figure().savefig("confusing_digit" + str(image) + ".png")  
</code></pre></div></div>

<p><img src="../images/confusing_digit7511.png" alt="My favorite bad MNIST digit." /></p>

<p>It‚Äôs classified as a 5, and as a human, I agree with that, but it‚Äôs also the worst 5 I have ever seen.</p>

<p>One hiccup happened when I went to write this up: I re-ran my code and got different images than before! Nondeterministic code makes for bad science, so I added the following four lines from <a href="https://machinelearningmastery.com/reproducible-results-neural-networks-keras/">this tutorial</a> to make my output the same every time. (The bad 5 above is part of the final reproducible set.)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from numpy.random import seed
seed(0)
from tensorflow import set_random_seed
set_random_seed(0)
</code></pre></div></div>

<p>Fortunately, the model is giving very sensible results, even with confusing input, and I can move on to my next tutorial ‚Äì perhaps reproducing the <a href="https://www.biorxiv.org/content/10.1101/262501v2.full">this paper</a> that analyzes epidermal stem cell development using neural networks. They also use Tensorflow, and they have a lovely <a href="https://github.com/luslab/scRNAseq-WGAN-GP">public code repository</a>.</p>

:ET