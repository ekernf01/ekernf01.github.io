---
layout: post
title: Expression forecasting benchmarks
math: true
permalink: perturbation-benchmarks
tags: GRN
---

I wrote about the [boom of new perturbation prediction methods](perturbation-methods). The natural predator of the methods developer is the benchmark developer, and a population boom of methods is naturally followed by a boom of benchmarks. (The usual prediction about what happens after the booms is left as an [exercise to the reader](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations).) Here are some benchmark studies that evaluate perturbation prediction methods, along with some thoughts about their distinctive advantages.

- [Ahlmann-Eltze et al](https://www.biorxiv.org/content/10.1101/2024.09.16.613342v3) focuses on comparing foundation models to simple baselines. It includes a peculiar linear baseline that beats GEARS, scGPT, and scFoundation almost uniformly when predicting genetic interactions on the Norman data or predicting novel perturbation outcomes on other perturb-seq datasets. This is an important finding, and the method is simple enough, but I hadn't seen it anywhere else. I don't think this work is intended to be re-used and extended by other teams aside from the authors, so my advice would be: read it; heed it; don't need to repeat it. If I hear otherwise I'll update this post.
- [PerturBench](https://arxiv.org/abs/2408.10609v1) targets slightly different tasks but uses a lot of the same ingredients. They find leading performance using simple, but nonlinear, latent-space arithmetic. Their work is very clearly meant to be reused, so if you're a methods developer looking for a quick way to access a lot of results, you should take a look. Their framework seems to be highly flexible, especially the data splitting: you can manually specify what goes in the test set. Their framework includes 3 ready-to-go datasets, two of which include genetic perturbations. Their way of incoporating new methods seems to be python-only, but if your method is written in R, Julia, or something else, maybe you can rig it up to call your method using a subprocess. I am not sure how to add new datasets -- I think that's a work in progress. This work took thousands of GPU hours and is distinguished by the breadth of coverage over deep-learning methods. 
- [CausalBench](https://arxiv.org/abs/2210.17283) has important findings in line with what I have seen in my own work: namely, that causal inference methods do not necessarily make better predictions than alternative methods or baselines with no underlying causal theory. This work is from an open challenge by GSK that is now over. It is not clear to me whether it is intended to still be used, but the interface does look very convenient, and since it was for a competition, you can be sure the interface has been tested by several independent teams. Their way of incoporating new methods seems to be python-only, but if your method is written in R, Julia, or something else, maybe you can rig it up to call your method using a subprocess. This work is mostly focused on network structure recovery. They do use held-out interventions in the test set, but I am not sure of the details. This framework offers two datasets that are ready to go. I am not sure how to add new datasets.
- [PEREGGRN](https://www.biorxiv.org/content/10.1101/2023.07.28.551039v2) constitutes the bulk of my PhD work, and therefore I cannot discuss it objectively. If I were to try, I would say the diversity of datasets is a big strength: we include many different cell types and many ways of inducing GoF/LoF, and we have validation code that helps users to add their own datasets with all the necessary structure. Also, the modularity is a big strength; new methods can be [added in Docker containers](https://github.com/ekernf01/pereggrn/blob/main/docs/how_to.md#how-to-evaluate-a-new-method), so you can use R, Python, Julia, or anything else. Also, we have very simple mean and median baselines that are surprisingly hard to beat. We were dismayed by this, obviously, but the baselines provide very important context.